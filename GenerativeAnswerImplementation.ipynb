{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T19:33:52.406835Z",
     "start_time": "2025-02-26T19:33:45.190140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchgen.api.cpp import return_type\n",
    "from transformers import BertTokenizer, T5ForConditionalGeneration, Text2TextGenerationPipeline, AutoTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "text2text_generator(\"中国的首都是extra0京\", max_length=50, do_sample=False)"
   ],
   "id": "919f29001b35ac01",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'extra0 北 extra1 extra2 extra3 extra4 extra5 extra6 extra7'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:05:46.706834Z",
     "start_time": "2025-02-26T23:05:46.692832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from rouge import Rouge\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "556c99feafbdc7dc",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T20:49:07.403152Z",
     "start_time": "2025-02-26T20:49:05.815091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/t5-base-chinese-cluecorpussmall')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('uer/t5-base-chinese-cluecorpussmall').to(device)\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "text2text_generator(\"中国的首都是extra0京\", max_length=50, do_sample=False)"
   ],
   "id": "91eefd7135b8a0df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/858M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b846e067d74b4d198ae988ae90fc8dc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'extra0 北 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 extra13 extra14 extra15 extra16 extra17 extra18 extra19 extra9'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T20:26:41.926408Z",
     "start_time": "2025-02-26T20:26:41.836878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read DuREaderQG data\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class DuReaderQG(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            idx = 0\n",
    "            for line in f:  # Read line by line\n",
    "                try:\n",
    "                    article = json.loads(line.strip())  # Parse each line as a JSON object\n",
    "                    context = article.get('context', '')\n",
    "                    question = article.get('question', '')\n",
    "                    answer = article.get('answer', '')\n",
    "                    q_id = article.get('id', idx)  # Use idx as fallback if 'id' is missing\n",
    "\n",
    "                    Data[idx] = {\n",
    "                        'id': q_id,\n",
    "                        'context': context,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                    }\n",
    "                    idx += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping invalid JSON line: {line}\\nError: {e}\")\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# Instantiate dataset (assuming JSON Lines format)\n",
    "train_data = DuReaderQG('data/DuReaderQG/train.json')\n",
    "valid_data = DuReaderQG('data/DuReaderQG/dev.json')\n",
    "test_data = valid_data\n",
    "# Print a sample\n",
    "print(train_data[0])\n"
   ],
   "id": "981571b8653d49e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'question': '仙剑奇侠传3第几集上天界', 'answer': '第35集'}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T20:26:53.692259Z",
     "start_time": "2025-02-26T20:26:53.679749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(valid_data)))"
   ],
   "id": "d1f3bc08c6394a91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 14520\n",
      "valid set size: 984\n",
      "test set size: 984\n",
      "{'id': 0, 'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'question': '2017年银行贷款基准利率', 'answer': '年基准利率4.35%'}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:02:19.522024Z",
     "start_time": "2025-02-26T23:02:19.200696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/t5-small-chinese-cluecorpussmall')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('uer/t5-small-chinese-cluecorpussmall').to(device)\n",
    "\n",
    "def collate_fn(batch_samples):\n",
    "    input_questions, input_contexts, batch_targets = [], [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        input_questions.append(sample['question'])\n",
    "        input_contexts.append(sample['context'])\n",
    "        batch_targets.append(sample['answer'])\n",
    "\n",
    "    batch_data = tokenizer(\n",
    "        input_questions,\n",
    "        input_contexts,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        batch_targets,\n",
    "        padding=True,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )[\"input_ids\"]\n",
    "    batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    batch_data[\"labels\"] = labels\n",
    "    batch_data.pop(\"token_type_ids\", None)  # Safe removal\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ],
   "id": "7c1c788a61a41edf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:02:20.177445Z",
     "start_time": "2025-02-26T23:02:20.165442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ],
   "id": "f7ec1ac210ee84d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([4, 290]), 'attention_mask': torch.Size([4, 290]), 'decoder_input_ids': torch.Size([4, 8]), 'labels': torch.Size([4, 8])}\n",
      "{'input_ids': tensor([[ 101, 3633, 1905,  ...,  828,  511,  102],\n",
      "        [ 101, 1798, 3332,  ...,    0,    0,    0],\n",
      "        [ 101, 9668, 7730,  ...,    0,    0,    0],\n",
      "        [ 101, 7167, 3354,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_input_ids': tensor([[ 101,  101, 8183, 2259,  102,    0,    0,    0],\n",
      "        [ 101,  101, 6205, 3959, 4277,  102,    0,    0],\n",
      "        [ 101,  101,  671, 2399,  671, 3613,  102,    0],\n",
      "        [ 101,  101, 3680, 2398, 3175, 8612, 2340, 1381]]), 'labels': tensor([[ 101, 8183, 2259,  102, -100, -100, -100, -100],\n",
      "        [ 101, 6205, 3959, 4277,  102, -100, -100, -100],\n",
      "        [ 101,  671, 2399,  671, 3613,  102, -100, -100],\n",
      "        [ 101, 3680, 2398, 3175, 8612, 2340, 1381,  102]])}\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:02:21.423754Z",
     "start_time": "2025-02-26T23:02:21.411755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = batch.to(device)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids[1]))"
   ],
   "id": "c1e57a2cad7b80ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '型', '材', '切', '割', '机', '什', '么', '牌', '子', '好', '[SEP]', '我', '觉', '得', '你', '可', '以', '买', '一', '台', '西', '湖', '牌', '的', '型', '材', '切', '割', '机', '，', '我', '们', '厂', '里', '去', '年', '买', '了', '一', '台', '吸', '尘', '型', '材', '切', '割', '机', '，', '效', '果', '还', '是', '比', '较', '满', '意', '的', '，', '吸', '尘', '效', '果', '比', '较', '好', '最', '重', '要', '的', '一', '点', '是', '安', '全', '，', '没', '有', '火', '花', '乱', '飞', '，', '当', '然', '价', '格', '不', '便', '宜', '，', '一', '等', '价', '钱', '一', '等', '货', '吧', '，', '这', '种', '机', '电', '产', '品', '还', '是', '买', '质', '量', '好', '的', '比', '较', '放', '心', '。', '看', '到', '了', '吗', '我', '们', '也', '是', '切', '割', '槽', '钢', '，', '角', '铁', '之', '类', '的', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:17:49.081304Z",
     "start_time": "2025-02-26T23:17:49.053789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(**inputs)\n",
    "print(inputs.input_ids.shape)\n",
    "print(output.logits.shape)"
   ],
   "id": "ab68a420be6941ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 290])\n",
      "torch.Size([4, 8, 21228])\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T22:59:42.992056Z",
     "start_time": "2025-02-26T22:59:42.458348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=32,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")\n",
    "string_output = tokenizer.decode(\n",
    "    generated_tokens[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(string_output)\n",
    "#not work well and can not generate any good results"
   ],
   "id": "8eb18e3a5e501f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra0 extra1 extra2 extra3 extra4 extra5 extra6 extra7 extra8 extra9 extra10 extra11 extra12 。 ？\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:10:32.932480Z",
     "start_time": "2025-02-26T23:10:32.918481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "bleu = BLEU()\n",
    "import numpy as np\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in [ 'Valid', 'Test']\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    return bleu.corpus_score(preds, labels).score\n"
   ],
   "id": "e4ca9ff18c1ce3d6",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T22:56:59.488803Z",
     "start_time": "2025-02-26T22:55:09.768297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loop(test_dataloader, model)\n",
    "#not work well"
   ],
   "id": "355cfaf282112550",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d909c499e6ce4cf39aca336a01a5426a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:05:51.442048Z",
     "start_time": "2025-02-26T23:05:51.427048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ],
   "id": "1918de2ade0b0486",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:07:53.598070Z",
     "start_time": "2025-02-26T23:07:53.583068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 2e-5\n",
    "epoch_num = 1\n",
    "beam_size = 4\n",
    "no_repeat_ngram_size = 2\n",
    "\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n"
   ],
   "id": "376129f4ae1b87f8",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:36:29.708796Z",
     "start_time": "2025-02-26T23:36:23.854117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_scheduler\n",
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "seed_everything(42)\n",
    "max_length = 128\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3\n",
    "\n",
    "class DuReaderQG(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            idx = 0\n",
    "            for line in f:  # Read line by line\n",
    "                try:\n",
    "                    article = json.loads(line.strip())\n",
    "                    context = article.get('context', '')\n",
    "                    question = article.get('question', '')\n",
    "                    answer = article.get('answer', '')\n",
    "                    q_id = article.get('id', idx)\n",
    "\n",
    "                    Data[idx] = {\n",
    "                        'id': q_id,\n",
    "                        'context': context,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                    }\n",
    "                    idx += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping invalid JSON line: {line}\\nError: {e}\")\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# Instantiate dataset (assuming JSON Lines format)\n",
    "train_data = DuReaderQG('data/DuReaderQG/train.json')\n",
    "valid_data = DuReaderQG('data/DuReaderQG/dev.json')\n",
    "test_data = valid_data\n",
    "\n",
    "## define model and tokenizer:\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('uer/t5-base-chinese-cluecorpussmall')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('uer/t5-base-chinese-cluecorpussmall').to(device)\n",
    "\n",
    "def collate_fn(batch_samples):\n",
    "    input_questions, input_contexts, batch_targets = [], [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        input_questions.append(sample['question'])\n",
    "        input_contexts.append(sample['context'])\n",
    "        batch_targets.append(sample['answer'])\n",
    "\n",
    "    batch_data = tokenizer(\n",
    "        input_questions,\n",
    "        input_contexts,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        batch_targets,\n",
    "        padding=True,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )[\"input_ids\"]\n",
    "    batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    batch_data[\"labels\"] = labels\n",
    "    batch_data.pop(\"token_type_ids\", None)  # Safe removal\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# define train loop and test loop:\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "# define test loop\n",
    "bleu = BLEU()\n",
    "\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in [ 'Valid', 'Test']\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    return bleu.corpus_score(preds, labels).score\n",
    "\n",
    "\n",
    "# define optimizer:\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "\n",
    "# main loop:\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model)\n",
    "    print(f\"BLEU: {valid_bleu:>0.2f}\\n\")\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f'epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin'\n",
    "        )\n",
    "print(\"Done!\")"
   ],
   "id": "a05c5f0eb8b12bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e390fdad1efa4978ace6865166505181"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 181\u001B[0m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 181\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m     valid_bleu \u001B[38;5;241m=\u001B[39m test_loop(valid_dataloader, model)\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBLEU: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalid_bleu\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m>0.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[86], line 129\u001B[0m, in \u001B[0;36mtrain_loop\u001B[1;34m(dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001B[0m\n\u001B[0;32m    127\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    128\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 129\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m lr_scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    132\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     73\u001B[0m instance\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     74\u001B[0m wrapped \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance, \u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    389\u001B[0m             )\n\u001B[1;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\optimization.py:696\u001B[0m, in \u001B[0;36mAdamW.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    693\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;66;03m# In-place operations to update the averages at the same time\u001B[39;00m\n\u001B[0;32m    695\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m beta1))\n\u001B[1;32m--> 696\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[0;32m    697\u001B[0m denom \u001B[38;5;241m=\u001B[39m exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt()\u001B[38;5;241m.\u001B[39madd_(group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    699\u001B[0m step_size \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T23:34:08.271304Z",
     "start_time": "2025-02-26T23:34:07.726102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_tokens = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=32,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")\n",
    "string_output = tokenizer.decode(\n",
    "    generated_tokens[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(string_output)"
   ],
   "id": "63a3ef17248e3500",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文 件 年 五 十 周 岁 。 的 不 一 周 ， 上 二 三 周 一\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66f172f4758c7390"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
