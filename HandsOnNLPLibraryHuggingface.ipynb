{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-22T23:11:55.647041Z",
     "start_time": "2025-02-22T23:11:53.824502Z"
    }
   },
   "source": [
    "from anyio.lowlevel import checkpoint\n",
    "from transformers import pipeline, AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:13:00.518485Z",
     "start_time": "2025-02-22T23:13:00.305468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)\n",
    "results = classifier(\n",
    "  [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")\n",
    "print(results)"
   ],
   "id": "e7181b6a354ebcaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598046541213989}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9598046541213989}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:15:09.202028Z",
     "start_time": "2025-02-22T23:14:52.143804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "results = classifier(\n",
    "    \"this is a course about trasnformers\",\n",
    "    candidate_labels = [\"education\", \"business\"]\n",
    ")\n",
    "print(results)"
   ],
   "id": "119fa053073b3934",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e2c123430f542d2849dbde6cf985103"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a20083ca5bf74505bbc3c84f0688c181"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c33a5a21ce364151950a8401b61b9a35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10e7f2bf34194d4c9d8c3eaf89532910"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e55befd56964f68a316a774dfd62350"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7b67a9e11c448b58aae92f3c173bc10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'this is a course about trasnformers', 'labels': ['education', 'business'], 'scores': [0.5915650129318237, 0.40843501687049866]}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:29:13.157177Z",
     "start_time": "2025-02-22T23:29:12.059514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "results = generator(\"I call my mother's mother \")\n",
    "print(results)\n",
    "results = generator(\n",
    "    \"I call my mother's mother \",\n",
    "    num_return_sequences=2,\n",
    "    max_length=50\n",
    ")\n",
    "print(results)"
   ],
   "id": "2de49eb2b9d569d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I call my mother\\'s mother ____.\"\\n\\nOn June 14, 2012, just like any other morning I wake with a low rumble at around 5 am. The doorbell rings. The person sitting in the corner opens the door, \"'}]\n",
      "[{'generated_text': \"I call my mother's mother 『Sama』, after all. She, like other humans that I have met before, has a hard time being affectionate, nor will she, like my mother 『Mom』, let alone the ones\"}, {'generated_text': 'I call my mother\\'s mother \\xa0(her maiden name) \\xa0in a song called \"Honey and Chocolat.\" Her last words when she\\'s talking with me are \\xa0\"I told you it wasn\\'t gonna be so bad'}]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:29:50.372186Z",
     "start_time": "2025-02-22T23:29:45.873576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "results = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(results)"
   ],
   "id": "8ca5de165d003d02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8667dbbf16d2488fb5d070cbb0bac94b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85382afb94164490acf87694fe84f95d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1ba202b32d64c2883980b29aad4667f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fcaf4443da64293abc05e7e7432a8b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7e8a41971d6488a95e8e4ddbb603f24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9039727f63cc420b9255cc8c3a4d20f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47db6585911f4b458cc3b9661a918b77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to write a program that enables you to write a Java implementation and program logic using Java code.\\n\\n'}, {'generated_text': 'In this course, we will teach you how to achieve an individual›. What› and what other students see is they are seeing the same'}]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:30:17.737344Z",
     "start_time": "2025-02-22T23:30:13.495641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "results = unmasker(\"This course will teach you all about <mask> models.\", top_k=2)\n",
    "print(results)"
   ],
   "id": "b9de717ea6e5ddb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1db5c0ef8e346b4aaa5c06704b7677e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fff33d309ed4ffcbdfc7132686e56c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0899ee55042a4ed1b7e6dc662c8e1f7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e04b3d970fa14c4583112ef12149e968"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fa9d4c8184549468eca38a1f84b001b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f75d503442a4708b6fc6930646d192a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.19198480248451233, 'token': 30412, 'token_str': ' mathematical', 'sequence': 'This course will teach you all about mathematical models.'}, {'score': 0.04209213703870773, 'token': 38163, 'token_str': ' computational', 'sequence': 'This course will teach you all about computational models.'}]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:45:07.184211Z",
     "start_time": "2025-02-22T23:45:06.382044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "results = ner(\"My name is Sylvain and I work at Hugging face in Brooklyn\")\n",
    "print(results)"
   ],
   "id": "410629c2b8df5690",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9986724, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.940561, 'word': 'Hugging face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.97993374, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T23:45:58.674445Z",
     "start_time": "2025-02-22T23:45:55.163232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question_answer = pipeline(\"question-answering\")\n",
    "answer = question_answer(\n",
    "    question=\"Where do I work\",\n",
    "    context=\"My name is Sylvain and I work at Hugging face in Brooklyn\",\n",
    ")\n",
    "print(answer)"
   ],
   "id": "7c868e21477b553a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5811a8b717a4148b9c84f738ca281ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb80bf58e99b498ab18839409a59c409"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ae0fe5f064b4ecba62ab89e19137028"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c4fd14bceb7410b869985d816469093"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ddabd118e1c4d9ca6f3a012b1460370"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5018935799598694, 'start': 33, 'end': 57, 'answer': 'Hugging face in Brooklyn'}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:32:21.712506Z",
     "start_time": "2025-02-23T03:32:20.657024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "results = summarizer(\n",
    "    '''\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "    '''\n",
    ")\n",
    "print(results)"
   ],
   "id": "d1760c3ed1faf23d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' China and India graduate six and eight times as many traditional engineers as the U.S. as does other industrial countries . America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:34:35.456486Z",
     "start_time": "2025-02-23T03:34:34.664720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "7b52a62a9f87caf4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bfe6a7df1e84de9a4e27a4c6a8addb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8acf21c2a91c4ba1ad8ee9ecf6d2b74f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ac4384337fb43a1aee7b843410e03b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:38:10.728335Z",
     "start_time": "2025-02-23T03:38:10.715334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_inputs = [\n",
    "    \"I like it\",\n",
    "    'I hate it'\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "id": "8e3f43b26f93efa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2066, 2009,  102],\n",
      "        [ 101, 1045, 5223, 2009,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:38:15.154143Z",
     "start_time": "2025-02-23T03:38:15.078743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ],
   "id": "4e9e4d22d4977802",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:38:58.043098Z",
     "start_time": "2025-02-23T03:38:58.017097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(**inputs)\n",
    "print(output)\n",
    "print(output[0].shape)\n",
    "print(output.last_hidden_state.shape)\n"
   ],
   "id": "efeacb11ef20f6f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.4636,  0.0641,  0.0654,  ...,  0.6416,  0.9967, -0.2961],\n",
      "         [ 0.7070,  0.1680,  0.0582,  ...,  0.5239,  1.0183, -0.2131],\n",
      "         [ 0.6558,  0.1891,  0.2555,  ...,  0.4995,  0.9141, -0.1542],\n",
      "         [ 0.4478, -0.0666,  0.1210,  ...,  0.6708,  0.8247, -0.1986],\n",
      "         [ 1.0028,  0.2052,  0.7059,  ...,  0.6399,  0.5203, -0.9045]],\n",
      "\n",
      "        [[-0.3134,  0.8145, -0.1955,  ..., -0.0663, -0.9960, -0.1345],\n",
      "         [-0.3028,  0.9973, -0.1145,  ..., -0.2894, -0.6548,  0.2221],\n",
      "         [-0.1717,  0.9030, -0.1645,  ..., -0.1247, -0.8535, -0.0032],\n",
      "         [-0.5075,  0.6215, -0.3129,  ..., -0.1344, -1.0326, -0.1878],\n",
      "         [ 0.1176,  0.5040, -0.2761,  ..., -0.1309, -0.7347, -0.2562]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([2, 5, 768])\n",
      "torch.Size([2, 5, 768])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:40:44.544216Z",
     "start_time": "2025-02-23T03:40:44.469415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
   ],
   "id": "47aaac5783d86e1c",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:41:45.840641Z",
     "start_time": "2025-02-23T03:41:45.812642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(**inputs)\n",
    "print(output.logits)"
   ],
   "id": "acdee7d4d80a50d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2899,  4.5791],\n",
      "        [ 4.3940, -3.5347]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:43:22.385089Z",
     "start_time": "2025-02-23T03:43:22.357604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(model(**inputs).logits, dim=-1 )\n",
    "print(predictions)"
   ],
   "id": "837d94d3f8c0926b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4067e-04, 9.9986e-01],\n",
      "        [9.9964e-01, 3.6016e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:53:37.285953Z",
     "start_time": "2025-02-23T03:53:32.491984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "d972bfb9919ea46a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74eee15c48de4faea33d10bd21faada7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "629a67bb0601491b87229eaa2a09d915"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:57:39.456680Z",
     "start_time": "2025-02-23T03:57:38.849274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "model.save_pretrained(\"./models/bert-base-cased/\")"
   ],
   "id": "45a9cf812e1bdf96",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T03:57:43.749592Z",
     "start_time": "2025-02-23T03:57:43.705407Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModel.from_pretrained(\"./models/bert-base-cased/\")",
   "id": "da0151cc8c79c3ca",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:03:26.538596Z",
     "start_time": "2025-02-23T04:03:26.465084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"./models/bert-base-cased/\")"
   ],
   "id": "af64bf722df0126f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/bert-base-cased/tokenizer_config.json',\n",
       " './models/bert-base-cased/special_tokens_map.json',\n",
       " './models/bert-base-cased/vocab.txt',\n",
       " './models/bert-base-cased/added_tokens.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:01:50.221233Z",
     "start_time": "2025-02-23T04:01:50.145806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"./models/bert-base-cased/\")"
   ],
   "id": "53a04868a86addd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/bert-base-cased/tokenizer_config.json',\n",
       " './models/bert-base-cased/special_tokens_map.json',\n",
       " './models/bert-base-cased/vocab.txt',\n",
       " './models/bert-base-cased/added_tokens.json',\n",
       " './models/bert-base-cased/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:03:27.795172Z",
     "start_time": "2025-02-23T04:03:27.789170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence = \"test the tokenizer is to tokenize the sequence\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ],
   "id": "e0852c827c8958fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'the', 'token', '##izer', 'is', 'to', 'token', '##ize', 'the', 'sequence']\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:03:44.111208Z",
     "start_time": "2025-02-23T04:03:44.097206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ],
   "id": "abe81c2ec560dcee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2774, 1103, 22559, 17260, 1110, 1106, 22559, 3708, 1103, 4954]\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:10:33.123595Z",
     "start_time": "2025-02-23T04:10:33.109554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_ids = tokenizer.encode(sequence)\n",
    "print(sequence_ids)"
   ],
   "id": "aff5bcf447b67f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2774, 1103, 22559, 17260, 1110, 1106, 22559, 3708, 1103, 4954, 102]\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:13:04.019376Z",
     "start_time": "2025-02-23T04:13:04.004865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# directly use tokenizer\n",
    "tokenized_text = tokenizer(sequence)\n",
    "print(tokenized_text)"
   ],
   "id": "9d9a585ca81411f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2774, 1103, 22559, 17260, 1110, 1106, 22559, 3708, 1103, 4954, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:15:11.126085Z",
     "start_time": "2025-02-23T04:15:11.112958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_string = tokenizer.decode(sequence_ids)\n",
    "print(decode_string)"
   ],
   "id": "cca554bf2affa43c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] test the tokenizer is to tokenize the sequence [SEP]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:15:54.996726Z",
     "start_time": "2025-02-23T04:15:54.982720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_string2 = tokenizer.decode(ids)\n",
    "print(decode_string2)"
   ],
   "id": "e513eab7f6873951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test the tokenizer is to tokenize the sequence\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:22:44.676183Z",
     "start_time": "2025-02-23T04:22:44.514392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# multi text , convert to batch , the model also just accept batch\n",
    "input_ids = torch.tensor([ids])\n",
    "print(input_ids)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "outputs = model(input_ids)\n",
    "print(outputs.logits)"
   ],
   "id": "7c8b1ccf791e9a13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2774,  1103, 22559, 17260,  1110,  1106, 22559,  3708,  1103,  4954]])\n",
      "tensor([[-0.8346,  0.0539]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:23:36.349067Z",
     "start_time": "2025-02-23T04:23:36.244431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "70ddd79e15805268",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7078, -0.1433]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6591, -0.1711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7078, -0.1433],\n",
      "        [-0.7962, -0.0011]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:25:50.981808Z",
     "start_time": "2025-02-23T04:25:50.887353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batched_attention_masks = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "outputs = model(\n",
    "    torch.tensor(batched_ids),\n",
    "    attention_mask=torch.tensor(batched_attention_masks))\n",
    "print(outputs.logits)"
   ],
   "id": "5639970b756db234",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7078, -0.1433]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6591, -0.1711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7078, -0.1433],\n",
      "        [-0.6591, -0.1711]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:27:32.372418Z",
     "start_time": "2025-02-23T04:27:32.280471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)"
   ],
   "id": "4c6933843a2a0283",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:27:43.999088Z",
     "start_time": "2025-02-23T04:27:43.923338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print(model_inputs)\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print(model_inputs)"
   ],
   "id": "1e242c049ccdc0d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:32:16.310924Z",
     "start_time": "2025-02-23T04:32:16.220632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences, max_length=8,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(model_inputs)"
   ],
   "id": "8e2d70bac3a40b7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005,  102],\n",
      "        [ 101, 2061, 2031, 1045,  999,  102,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:32:51.587508Z",
     "start_time": "2025-02-23T04:32:51.384618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ],
   "id": "b8c21672e6eb72ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:36:39.592793Z",
     "start_time": "2025-02-23T04:36:39.504633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(inputs)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "print(tokens)\n",
    "\n"
   ],
   "id": "59132a83161a672b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:37:33.117723Z",
     "start_time": "2025-02-23T04:37:33.041347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence1_list = [\"First sentence.\", \"This is the second sentence.\", \"Third one.\"]\n",
    "sentence2_list = [\"First sentence is short.\", \"The second sentence is very very very long.\", \"ok.\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sentence1_list,\n",
    "    sentence2_list,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokens)\n",
    "print(tokens['input_ids'].shape)"
   ],
   "id": "ee77bdce81c1d7c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,\n",
      "         2200, 2200, 2200, 2146, 1012,  102],\n",
      "        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([3, 18])\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:39:39.405189Z",
     "start_time": "2025-02-23T04:39:39.344437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence = 'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'\n",
    "print(tokenizer.tokenize(sentence))"
   ],
   "id": "7aded059a0ba4b82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', '[', 'en', '##t', '_', 'start', ']', 'cars', '[', 'en', '##t', '_', 'end', ']', 'collided', 'in', 'a', '[', 'en', '##t', '_', 'start', ']', 'tunnel', '[', 'en', '##t', '_', 'end', ']', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:40:43.218918Z",
     "start_time": "2025-02-23T04:40:43.207895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_added_toks = tokenizer.add_tokens([\"new_token1\", \"my_new-token2\"])\n",
    "print(\"We have added\", num_added_toks, \"tokens\")"
   ],
   "id": "e8d95fd3b123c145",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 0 tokens\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:40:54.764999Z",
     "start_time": "2025-02-23T04:40:54.750463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_tokens = [\"new_token1\", \"my_new-token2\"]\n",
    "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "tokenizer.add_tokens(list(new_tokens))"
   ],
   "id": "7b06ede1fdd83b6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:41:57.901005Z",
     "start_time": "2025-02-23T04:41:57.890988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens_dict = {\"cls_token\": \"[MY_CLS]\"}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(\"We have added\", num_added_toks, \"tokens\")\n",
    "assert tokenizer.cls_token == \"[MY_CLS]\""
   ],
   "id": "ac0afcc0deb70480",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 0 tokens\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:43:50.657345Z",
     "start_time": "2025-02-23T04:43:50.549780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)\n",
    "# num_added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[ENT_START]', '[ENT_END]']})\n",
    "print(\"We have added\", num_added_toks, \"tokens\")\n",
    "\n",
    "sentence = 'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'"
   ],
   "id": "54551473c185a1fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 2 tokens\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:44:31.689276Z",
     "start_time": "2025-02-23T04:44:31.191526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)\n",
    "print(\"After we add\", num_added_toks, \"tokens\")\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.embeddings.word_embeddings.weight.size())\n",
    "\n",
    "# Randomly generated matrix\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "1a01561302962363",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 30522\n",
      "After we add 2 tokens\n",
      "vocabulary size: 30524\n",
      "torch.Size([30524, 768])\n",
      "tensor([[-0.0221, -0.0379, -0.0354,  ..., -0.0287, -0.0341, -0.0267],\n",
      "        [-0.0221, -0.0379, -0.0354,  ..., -0.0287, -0.0341, -0.0267]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:46:41.692642Z",
     "start_time": "2025-02-23T04:46:41.680631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# manually initialize the new token embedding\n",
    "\n",
    "token_id = tokenizer.convert_tokens_to_ids('entity')\n",
    "token_embedding = model.embeddings.word_embeddings.weight[token_id]\n",
    "print(token_id)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, num_added_toks+1):\n",
    "        model.embeddings.word_embeddings.weight[-i:, :] = token_embedding.clone().detach().requires_grad_(True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "2b4d34654c5cb314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9178\n",
      "tensor([[-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419],\n",
      "        [-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T04:49:56.008318Z",
     "start_time": "2025-02-23T04:49:55.995299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize the exist token\n",
    "descriptions = ['start of entity', 'end of entity']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, token in enumerate(reversed(descriptions), start=1):\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        print(tokenized)\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        new_embedding = model.embeddings.word_embeddings.weight[tokenized_ids].mean(axis=0)\n",
    "        model.embeddings.word_embeddings.weight[-i, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "706a8e0125066e82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end', 'of', 'entity']\n",
      "['start', 'of', 'entity']\n",
      "tensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],\n",
      "        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:34:45.956851Z",
     "start_time": "2025-02-23T21:34:45.947335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        assert end > start\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.end))\n",
    "\n",
    "ds = MyIterableDataset(start=3, end=7) # [3, 4, 5, 6]\n",
    "# Single-process loading\n",
    "print(list(DataLoader(ds, num_workers=0)))\n",
    "# Directly doing multi-process loading\n",
    "print(list(DataLoader(ds, num_workers=0)))"
   ],
   "id": "f1150a14f3aae3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n",
      "[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:39:53.606544Z",
     "start_time": "2025-02-23T21:39:48.398343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import get_worker_info\n",
    "import math\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = get_worker_info()\n",
    "    dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "    overall_start = dataset.start\n",
    "    overall_end = dataset.end\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "    worker_id = worker_info.id\n",
    "    dataset.start = overall_start + worker_id * per_worker\n",
    "    dataset.end = min(dataset.start + per_worker, overall_end)\n",
    "\n",
    "# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
    "print(list(DataLoader(ds, num_workers=0, worker_init_fn=worker_init_fn)))\n",
    "# With even more workers\n",
    "print(list(DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))"
   ],
   "id": "cf3a1fc16dc096c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 19392, 5480, 4236, 26852, 11244, 23460, 35828, 3380, 8052, 8408, 34492, 33580, 21564, 6920, 33440, 30092, 3208, 35184, 17940, 15008) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1133\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\multiprocessing\\queues.py:114\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[1;32m--> 114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlist\u001B[39m(DataLoader(ds, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, worker_init_fn\u001B[38;5;241m=\u001B[39mworker_init_fn)))\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# With even more workers\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_init_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworker_init_fn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1329\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1332\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1291\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1292\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1294\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1295\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1296\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1297\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1145\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[1;32m-> 1146\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpids_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[0;32m   1148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 19392, 5480, 4236, 26852, 11244, 23460, 35828, 3380, 8052, 8408, 34492, 33580, 21564, 6920, 33440, 30092, 3208, 35184, 17940, 15008) exited unexpectedly"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:41:33.540631Z",
     "start_time": "2025-02-23T21:41:31.007997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ],
   "id": "3c438aceb0c07739",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:43:07.456346Z",
     "start_time": "2025-02-23T21:43:01.903714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "print(img.shape)\n",
    "print(f\"Label: {label}\")"
   ],
   "id": "31d6d6ccca671cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 11445971.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 341728.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 5241350.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "torch.Size([28, 28])\n",
      "Label: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:46:33.950923Z",
     "start_time": "2025-02-23T21:46:33.924919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SequentialSampler, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(training_data)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, sampler=train_sampler)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, sampler=test_sampler)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "test_features, test_labels = next(iter(test_dataloader))\n",
    "print(f\"Feature batch shape: {test_features.size()}\")\n",
    "print(f\"Labels batch shape: {test_labels.size()}\")\n",
    "label = train_labels[0]\n",
    "print(f\"Label: {label}\")"
   ],
   "id": "b38500ac382347b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "Label: 1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:55:56.475963Z",
     "start_time": "2025-02-23T21:55:41.212111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=-1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "id": "7f8117da73d29a5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.771196  [ 6400/60000]\n",
      "loss: 0.886460  [12800/60000]\n",
      "loss: 0.656567  [19200/60000]\n",
      "loss: 0.945273  [25600/60000]\n",
      "loss: 0.513770  [32000/60000]\n",
      "loss: 0.586043  [38400/60000]\n",
      "loss: 0.886458  [44800/60000]\n",
      "loss: 0.554964  [51200/60000]\n",
      "loss: 0.603633  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.436605 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.527233  [ 6400/60000]\n",
      "loss: 0.725129  [12800/60000]\n",
      "loss: 0.509011  [19200/60000]\n",
      "loss: 0.897937  [25600/60000]\n",
      "loss: 0.390276  [32000/60000]\n",
      "loss: 0.531467  [38400/60000]\n",
      "loss: 0.750610  [44800/60000]\n",
      "loss: 0.440960  [51200/60000]\n",
      "loss: 0.461065  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.394805 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.599038  [ 6400/60000]\n",
      "loss: 0.542877  [12800/60000]\n",
      "loss: 0.552482  [19200/60000]\n",
      "loss: 0.777652  [25600/60000]\n",
      "loss: 0.288527  [32000/60000]\n",
      "loss: 0.441646  [38400/60000]\n",
      "loss: 0.875145  [44800/60000]\n",
      "loss: 0.460651  [51200/60000]\n",
      "loss: 0.557235  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.418014 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3a97d2094dc9808"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
