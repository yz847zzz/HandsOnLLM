{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T19:56:31.077303Z",
     "start_time": "2025-02-25T19:56:30.940504Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class CMRC2018(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "            idx = 0\n",
    "            for article in json_data['data']:\n",
    "                title = article['title']\n",
    "                context = article['paragraphs'][0]['context']\n",
    "                for question in article['paragraphs'][0]['qas']:\n",
    "                    q_id = question['id']\n",
    "                    ques = question['question']\n",
    "                    text = [ans['text'] for ans in question['answers']]\n",
    "                    answer_start = [ans['answer_start'] for ans in question['answers']]\n",
    "                    Data[idx] = {\n",
    "                        'id': q_id,\n",
    "                        'title': title,\n",
    "                        'context': context,\n",
    "                        'question': ques,\n",
    "                        'answers': {\n",
    "                            'text': text,\n",
    "                            'answer_start': answer_start\n",
    "                        }\n",
    "                    }\n",
    "                    idx += 1\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = CMRC2018('data/cmrc2018/cmrc2018_train.json')\n",
    "valid_data = CMRC2018('data/cmrc2018/cmrc2018_dev.json')\n",
    "test_data = CMRC2018('data/cmrc2018/cmrc2018_trial.json')"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:56:43.462395Z",
     "start_time": "2025-02-25T19:56:43.448375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(valid_data)))"
   ],
   "id": "6c3626acf8253ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 10142\n",
      "valid set size: 3219\n",
      "test set size: 1002\n",
      "{'id': 'DEV_0_QUERY_0', 'title': '战国无双3', 'context': '《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品', 'question': '《战国无双3》是由哪两个公司合作开发的？', 'answers': {'text': ['光荣和ω-force', '光荣和ω-force', '光荣和ω-force'], 'answer_start': [11, 11, 11]}}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:00:40.807293Z",
     "start_time": "2025-02-25T20:00:40.668300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "9db2732470e3c1b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:03:13.856919Z",
     "start_time": "2025-02-25T20:03:13.844919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#we handle the context into : question + text= [CLS]question[SEP]context[SEP]\n",
    "#Example:\n",
    "context = [train_data.data[i]['context'] for i in range(4)]\n",
    "question = [train_data.data[i]['question'] for i in range(4)]\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=300,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "print(inputs.keys())\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ],
   "id": "a446e48c54431796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
      "The 4 examples gave 14 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:04:43.952205Z",
     "start_time": "2025-02-25T20:04:43.939206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answers = [train_data[idx][\"answers\"] for idx in range(4)]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "print(answers)"
   ],
   "id": "d3526ad8158a0b84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ['1963年'], 'answer_start': [30]}, {'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]}, {'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]}, {'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]}]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:18:38.699531Z",
     "start_time": "2025-02-25T20:18:38.687531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "print(start_positions)\n",
    "print(end_positions)"
   ],
   "id": "3b410f180b3d8b84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0, 47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0, 47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0, 47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0, 47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0, 47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0]\n",
      "[48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0, 48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0, 48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0, 48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0, 48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0, 48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:18:55.488115Z",
     "start_time": "2025-02-25T20:18:55.474115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ],
   "id": "beb41ef8e0bf902f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: 1963年, labels give: 1963 年\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:19:48.198804Z",
     "start_time": "2025-02-25T20:19:48.185799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def train_collote_fn(batch_samples):\n",
    "    batch_question, batch_context, batch_answers = [], [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_question.append(sample['question'])\n",
    "        batch_context.append(sample['context'])\n",
    "        batch_answers.append(sample['answers'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_question,\n",
    "        batch_context,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length',\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = batch_data.pop('offset_mapping')\n",
    "    sample_mapping = batch_data.pop('overflow_to_sample_mapping')\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer = batch_answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
    "        sequence_ids = batch_data.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    return batch_data, torch.tensor(start_positions), torch.tensor(end_positions)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=train_collote_fn)"
   ],
   "id": "5945212ff14ffb2f",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:20:23.564907Z",
     "start_time": "2025-02-25T20:19:59.948093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "batch_X, batch_Start, batch_End = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_Start shape:', batch_Start.shape)\n",
    "print('batch_End shape:', batch_End.shape)\n",
    "print(batch_X)\n",
    "print(batch_Start)\n",
    "print(batch_End)\n",
    "\n",
    "print('train set size: ', )\n",
    "print(len(train_data), '->', sum([batch_data['input_ids'].shape[0] for batch_data, _, _ in train_dataloader]))"
   ],
   "id": "babbc2eea1a762fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([9, 384]), 'token_type_ids': torch.Size([9, 384]), 'attention_mask': torch.Size([9, 384])}\n",
      "batch_Start shape: torch.Size([9])\n",
      "batch_End shape: torch.Size([9])\n",
      "{'input_ids': tensor([[ 101, 6205, 5401,  ..., 1075,  511,  102],\n",
      "        [ 101, 6205, 5401,  ...,  671,  763,  102],\n",
      "        [ 101, 6205, 5401,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5276, 5432,  ...,  872, 2199,  102],\n",
      "        [ 101, 5276, 5432,  ..., 4638, 1355,  102],\n",
      "        [ 101, 5276, 5432,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([251,   0,   0, 203, 230,   0,  32,   0,   0])\n",
      "tensor([273,   0,   0, 217, 251,   0,  33,   0,   0])\n",
      "train set size: \n",
      "10142 -> 18960\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T21:14:40.432147Z",
     "start_time": "2025-02-25T21:14:40.418153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for test we just care about the text of answer ; but not sart/end positiom, so handle the data loader:\n",
    "def test_collote_fn(batch_samples):\n",
    "    batch_id, batch_question, batch_context = [], [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_id.append(sample['id'])\n",
    "        batch_question.append(sample['question'])\n",
    "        batch_context.append(sample['context'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_question,\n",
    "        batch_context,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = batch_data.pop('offset_mapping').numpy().tolist()\n",
    "    sample_mapping = batch_data.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(batch_data['input_ids'])):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        example_ids.append(batch_id[sample_idx])\n",
    "\n",
    "        sequence_ids = batch_data.sequence_ids(i)\n",
    "        offset = offset_mapping[i]\n",
    "        offset_mapping[i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "    return batch_data, offset_mapping, example_ids\n",
    "\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=8, shuffle=False, collate_fn=test_collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=test_collote_fn)\n"
   ],
   "id": "c73fbb98490e7ee7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:31:58.087640Z",
     "start_time": "2025-02-25T20:31:55.153421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_X, offset_mapping, example_ids = next(iter(valid_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print(example_ids)\n",
    "\n",
    "print('valid set size: ')\n",
    "print(len(valid_data), '->', sum([batch_data['input_ids'].shape[0] for batch_data, _, _ in valid_dataloader]))"
   ],
   "id": "a8e7b2d0a43b3e68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([16, 384]), 'token_type_ids': torch.Size([16, 384]), 'attention_mask': torch.Size([16, 384])}\n",
      "['DEV_0_QUERY_0', 'DEV_0_QUERY_0', 'DEV_0_QUERY_1', 'DEV_0_QUERY_1', 'DEV_0_QUERY_2', 'DEV_0_QUERY_2', 'DEV_1_QUERY_0', 'DEV_1_QUERY_0', 'DEV_1_QUERY_1', 'DEV_1_QUERY_1', 'DEV_1_QUERY_2', 'DEV_1_QUERY_2', 'DEV_1_QUERY_3', 'DEV_1_QUERY_3', 'DEV_2_QUERY_0', 'DEV_2_QUERY_0']\n",
      "valid set size: \n",
      "3219 -> 6254\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:34:34.593483Z",
     "start_time": "2025-02-25T20:34:32.438672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build model:\n",
    "# BERT + fc : where the fc layer output the position of start position and end position\n",
    "# 2 output, start: each token as start position's score\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForExtractiveQA(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForExtractiveQA.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)"
   ],
   "id": "d7fed765ef5bf56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForExtractiveQA were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "BertForExtractiveQA(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:35:36.562605Z",
     "start_time": "2025-02-25T20:35:36.351717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=train_collote_fn)\n",
    "\n",
    "batch_X, _, _ = next(iter(train_dataloader))\n",
    "start_outputs, end_outputs = model(batch_X.to(device))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('start_outputs shape', start_outputs.shape)\n",
    "print('end_outputs shape', end_outputs.shape)"
   ],
   "id": "c3aee98dd0d8af38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([7, 384]), 'token_type_ids': torch.Size([7, 384]), 'attention_mask': torch.Size([7, 384])}\n",
      "start_outputs shape torch.Size([7, 384])\n",
      "end_outputs shape torch.Size([7, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:36:42.449672Z",
     "start_time": "2025-02-25T20:36:42.435672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, start_pos, end_pos) in enumerate(dataloader, start=1):\n",
    "        X, start_pos, end_pos = X.to(device), start_pos.to(device), end_pos.to(device)\n",
    "        start_pred, end_pred = model(X)\n",
    "        start_loss = loss_fn(start_pred, start_pos)\n",
    "        end_loss = loss_fn(end_pred, end_pos)\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ],
   "id": "fd5a87d9de02783c",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:37:24.310102Z",
     "start_time": "2025-02-25T20:37:17.792056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "valid_data = CMRC2018('data/cmrc2018/cmrc2018_dev.json')\n",
    "small_eval_set = [valid_data[idx] for idx in range(12)]\n",
    "\n",
    "trained_checkpoint = \"uer/roberta-base-chinese-extractive-qa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = DataLoader(small_eval_set, batch_size=4, shuffle=False, collate_fn=test_collote_fn)\n",
    "\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)"
   ],
   "id": "5b6a73cd9af6420",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef4e264bbaea46fa99605a7bd087aa86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuanZ\\.cache\\huggingface\\hub\\models--uer--roberta-base-chinese-extractive-qa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cf0771dfd904684a00411c2cf0b7687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a78d845d8244ac59cf714e3cc99e10b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9906b7ea37dd465daad4e1f67f064f2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4023a09f62b6480ea62908bb018db08a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/407M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8b3227c6d4748cfbafbfe9f2f2c7572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:37:35.396017Z",
     "start_time": "2025-02-25T20:37:34.852668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_logits = []\n",
    "end_logits = []\n",
    "\n",
    "trained_model.eval()\n",
    "for batch_data, _, _ in eval_set:\n",
    "    batch_data = batch_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(**batch_data)\n",
    "    start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "    end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "import numpy as np\n",
    "start_logits = np.concatenate(start_logits)\n",
    "end_logits = np.concatenate(end_logits)"
   ],
   "id": "151f348b39c580d0",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:38:01.134943Z",
     "start_time": "2025-02-25T20:38:01.089432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_example_ids = []\n",
    "all_offset_mapping = []\n",
    "for _, offset_mapping, example_ids in eval_set:\n",
    "    all_example_ids += example_ids\n",
    "    all_offset_mapping += offset_mapping\n",
    "\n",
    "import collections\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature_id in enumerate(all_example_ids):\n",
    "    example_to_features[feature_id].append(idx)\n",
    "\n",
    "print(example_to_features)\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = all_offset_mapping[feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                if (end_index < start_index or end_index - start_index + 1 > max_answer_length):\n",
    "                    continue\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"start\": offsets[start_index][0],\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "    if len(answers) > 0:\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted_answers.append({\n",
    "            \"id\": example_id,\n",
    "            \"prediction_text\": best_answer[\"text\"],\n",
    "            \"answer_start\": best_answer[\"start\"]\n",
    "        })\n",
    "    else:\n",
    "        predicted_answers.append({\n",
    "            \"id\": example_id,\n",
    "            \"prediction_text\": \"\",\n",
    "            \"answer_start\": 0\n",
    "        })"
   ],
   "id": "eb1bccce2e7d625d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'DEV_0_QUERY_0': [0, 1], 'DEV_0_QUERY_1': [2, 3], 'DEV_0_QUERY_2': [4, 5], 'DEV_1_QUERY_0': [6, 7], 'DEV_1_QUERY_1': [8, 9], 'DEV_1_QUERY_2': [10, 11], 'DEV_1_QUERY_3': [12, 13], 'DEV_2_QUERY_0': [14, 15], 'DEV_2_QUERY_1': [16, 17], 'DEV_2_QUERY_2': [18, 19], 'DEV_3_QUERY_0': [20], 'DEV_3_QUERY_1': [21]})\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:38:07.357968Z",
     "start_time": "2025-02-25T20:38:07.344968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for pred, label in zip(predicted_answers, theoretical_answers):\n",
    "    print(pred['id'])\n",
    "    print('pred:', pred['prediction_text'])\n",
    "    print('label:', label['answers']['text'])"
   ],
   "id": "2621897b4e969f4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV_0_QUERY_0\n",
      "pred: 光荣和ω-force\n",
      "label: ['光荣和ω-force', '光荣和ω-force', '光荣和ω-force']\n",
      "DEV_0_QUERY_1\n",
      "pred: 任天堂游戏谜之村雨城\n",
      "label: ['村雨城', '村雨城', '任天堂游戏谜之村雨城']\n",
      "DEV_0_QUERY_2\n",
      "pred: 「战史演武」&「争霸演武」\n",
      "label: ['「战史演武」&「争霸演武」', '「战史演武」&「争霸演武」', '「战史演武」&「争霸演武」']\n",
      "DEV_1_QUERY_0\n",
      "pred: 锣鼓经是大陆传统器乐及戏曲里面常用的打击乐记谱方法\n",
      "label: ['大陆传统器乐及戏曲里面常用的打击乐记谱方法', '大陆传统器乐及戏曲里面常用的打击乐记谱方法', '大陆传统器乐及戏曲里面常用的打击乐记谱方法']\n",
      "DEV_1_QUERY_1\n",
      "pred: 「锣鼓点」\n",
      "label: ['锣鼓点', '锣鼓点', '锣鼓点']\n",
      "DEV_1_QUERY_2\n",
      "pred: 依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。\n",
      "label: ['依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。', '依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。', '依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点']\n",
      "DEV_1_QUERY_3\n",
      "pred: 戏曲锣鼓所运用的敲击乐器主要分为鼓、锣、钹和板四类型\n",
      "label: ['鼓、锣、钹和板', '鼓、锣、钹和板', '鼓、锣、钹和板']\n",
      "DEV_2_QUERY_0\n",
      "pred: 全长364.6公里\n",
      "label: ['364.6公里', '364.6公里', '364.6公里']\n",
      "DEV_2_QUERY_1\n",
      "pred: 三茂铁路股份有限公司\n",
      "label: ['三茂铁路股份有限公司', '三茂铁路股份有限公司', '三茂铁路股份有限公司']\n",
      "DEV_2_QUERY_2\n",
      "pred: 1903年\n",
      "label: ['1903年', '1903年', '1903年']\n",
      "DEV_3_QUERY_0\n",
      "pred: 山东省北部环渤海地区\n",
      "label: ['山东省北部环渤海地区', '山东省北部环渤海地区', '山东省北部环渤海地区']\n",
      "DEV_3_QUERY_1\n",
      "pred: 11.42亿元\n",
      "label: ['11.42亿元', '11.42亿元', '11.42亿元']\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:46:31.561373Z",
     "start_time": "2025-02-25T20:46:30.696397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cmrc2018_evaluate import evaluate\n",
    "result = evaluate(predicted_answers, theoretical_answers)\n",
    "print(f\"F1: {result['f1']:>0.2f} EM: {result['em']:>0.2f} AVG: {result['avg']:>0.2f}\\n\")"
   ],
   "id": "a6ed7701c6904546",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 92.63 EM: 75.00 AVG: 83.81\n",
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T20:47:50.801537Z",
     "start_time": "2025-02-25T20:47:50.787538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "from cmrc2018_evaluate import evaluate\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, start_pos, end_pos) in enumerate(dataloader, start=1):\n",
    "        X, start_pos, end_pos = X.to(device), start_pos.to(device), end_pos.to(device)\n",
    "        start_pred, end_pred = model(X)\n",
    "        start_loss = loss_fn(start_pred, start_pos)\n",
    "        end_loss = loss_fn(end_pred, end_pos)\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ],
   "id": "f9bcd4b0dde3a03e",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T21:06:26.097924Z",
     "start_time": "2025-02-25T20:48:41.290230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers import AdamW, get_scheduler\n",
    "import json\n",
    "import collections\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "sys.path.append('./')\n",
    "from cmrc2018_evaluate import evaluate\n",
    "def test_loop(dataloader, dataset, model):\n",
    "    all_example_ids = []\n",
    "    all_offset_mapping = []\n",
    "    for _, offset_mapping, example_ids in dataloader:\n",
    "        all_example_ids += example_ids\n",
    "        all_offset_mapping += offset_mapping\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature_id in enumerate(all_example_ids):\n",
    "        example_to_features[feature_id].append(idx)\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    model.eval()\n",
    "    for batch_data, _, _ in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred_start_logits, pred_end_logit = model(batch_data)\n",
    "        start_logits.append(pred_start_logits.cpu().numpy())\n",
    "        end_logits.append(pred_end_logit.cpu().numpy())\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "\n",
    "    theoretical_answers = [\n",
    "        {\"id\": dataset[s_idx][\"id\"], \"answers\": dataset[s_idx][\"answers\"]} for s_idx in range(len(dataset))\n",
    "    ]\n",
    "    predicted_answers = []\n",
    "    for s_idx in tqdm(range(len(dataset))):\n",
    "        example_id = dataset[s_idx][\"id\"]\n",
    "        context = dataset[s_idx][\"context\"]\n",
    "        answers = []\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = all_offset_mapping[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if (end_index < start_index or end_index-start_index+1 > max_answer_length):\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"start\": offsets[start_index][0],\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    })\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append({\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": best_answer[\"text\"],\n",
    "                \"answer_start\": best_answer[\"start\"]\n",
    "            })\n",
    "        else:\n",
    "            predicted_answers.append({\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": \"\",\n",
    "                \"answer_start\": 0\n",
    "            })\n",
    "    result = evaluate(predicted_answers, theoretical_answers)\n",
    "    print(f\"F1: {result['f1']:>0.2f} EM: {result['em']:>0.2f} AVG: {result['avg']:>0.2f}\\n\")\n",
    "    return result\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_avg_score = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_scores = test_loop(valid_dataloader, valid_data, model)\n",
    "    avg_score = valid_scores['avg']\n",
    "    if avg_score > best_avg_score:\n",
    "        best_avg_score = avg_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_avg_{avg_score:0.4f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ],
   "id": "1a418ad1950d32f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2536 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "087b23c22fed461b8e2cb4e02a30bcdb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f284bd060f14d64bdd61f5ebdd59ad3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3219 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6414c63bf6e245789185d97720d663a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 83.38 EM: 62.50 AVG: 72.94\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2536 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98696d94ea274afa9810fb9b01efcaf8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5796532105de4f5c966e46f808ff6c53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3219 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3b7d9bb642b4c65ad78f3d9c89525ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 84.88 EM: 65.58 AVG: 75.23\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2536 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07de40a3cc374ed8ada7075db3e60f4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fac95d885db84092a660e01570e1a3fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3219 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "519fa93841f64ae1beb7a9901bfb2698"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 84.63 EM: 64.27 AVG: 74.45\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T21:14:55.317628Z",
     "start_time": "2025-02-25T21:14:44.188562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load('epoch_2_valid_avg_75.2314_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    all_example_ids = []\n",
    "    all_offset_mapping = []\n",
    "    for _, offset_mapping, example_ids in test_dataloader:\n",
    "        all_example_ids += example_ids\n",
    "        all_offset_mapping += offset_mapping\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature_id in enumerate(all_example_ids):\n",
    "        example_to_features[feature_id].append(idx)\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    model.eval()\n",
    "    for batch_data, _, _ in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        pred_start_logits, pred_end_logit = model(batch_data)\n",
    "        start_logits.append(pred_start_logits.cpu().numpy())\n",
    "        end_logits.append(pred_end_logit.cpu().numpy())\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "\n",
    "    theoretical_answers = [\n",
    "        {\"id\": test_data[s_idx][\"id\"], \"answers\": test_data[s_idx][\"answers\"]} for s_idx in range(len(test_dataloader))\n",
    "    ]\n",
    "    predicted_answers = []\n",
    "    save_resluts = []\n",
    "    for s_idx in tqdm(range(len(test_data))):\n",
    "        example_id = test_data[s_idx][\"id\"]\n",
    "        context = test_data[s_idx][\"context\"]\n",
    "        title = test_data[s_idx][\"title\"]\n",
    "        question = test_data[s_idx][\"question\"]\n",
    "        labels = test_data[s_idx][\"answers\"]\n",
    "        answers = []\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = all_offset_mapping[feature_index]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if (end_index < start_index or end_index-start_index+1 > max_answer_length):\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"start\": offsets[start_index][0],\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append({\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": best_answer[\"text\"],\n",
    "                \"answer_start\": best_answer[\"start\"]\n",
    "            })\n",
    "            save_resluts.append({\n",
    "                \"id\": example_id,\n",
    "                \"title\": title,\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "                \"answers\": labels,\n",
    "                \"prediction_text\": best_answer[\"text\"],\n",
    "                \"answer_start\": best_answer[\"start\"]\n",
    "            })\n",
    "        else:\n",
    "            predicted_answers.append({\n",
    "                \"id\": example_id,\n",
    "                \"prediction_text\": \"\",\n",
    "                \"answer_start\": 0\n",
    "            })\n",
    "            save_resluts.append({\n",
    "                \"id\": example_id,\n",
    "                \"title\": title,\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "                \"answers\": labels,\n",
    "                \"prediction_text\": \"\",\n",
    "                \"answer_start\": 0\n",
    "            })\n",
    "    eval_result = evaluate(predicted_answers, theoretical_answers)\n",
    "    print(f\"F1: {eval_result['f1']:>0.2f} EM: {eval_result['em']:>0.2f} AVG: {eval_result['avg']:>0.2f}\\n\")\n",
    "    print('saving predicted results...')\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for example_result in save_resluts:\n",
    "            f.write(json.dumps(example_result, ensure_ascii=False) + '\\n')"
   ],
   "id": "fd39e5158e50cc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "817363423bbf4fb4bbbb58ed367ebd6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1002 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "148e0a086b674be89b878ec9d803afad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 73.01 EM: 32.54 AVG: 52.77\n",
      "\n",
      "saving predicted results...\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "310d4a535d02d013"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
