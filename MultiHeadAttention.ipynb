{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T02:29:39.199251Z",
     "start_time": "2025-02-19T02:29:39.140196Z"
    }
   },
   "source": [
    "'''\n",
    "batch size is 64; k words; 300 dim for each words\n",
    "k:[64, 10 ,300];\n",
    "v:[64, 10 ,300]\n",
    "q:[64, 12 ,300]\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hid_dim,n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        #force head dim max full divide by heads\n",
    "        assert hid_dim % n_heads == 0\n",
    "        # define weight Q\n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim// n_heads]))\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bsz = q.shape[0]\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(k)\n",
    "        V = self.w_v(v)\n",
    "\n",
    "        #divide the dim into n_heads attention groups\n",
    "        #K :[64, 10, 300] - > [64, 10, 6,50]  divide into 6 attention groups. each group will have 50 dim\n",
    "        # and then permutate the attention group 6 to the from, 10 words 50 dim to the back to calculate convenience.\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        #step 1. Q*K'/ scale : [64, 6,12,50] * [64, 6, 50,10] = [64,6, 12,10]\n",
    "        #attention = [64,6, 12,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2))/self.scale  #K' is transfer the last two dim as matrix\n",
    "\n",
    "        # if mask is not None; then mark the attention on the positiion at mask = 0 to -1e10 (means that can not been attentioned like padding unk)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # step2; softmax and dropout\n",
    "        attention = self.do(torch.softmax(attention, dim=-1))\n",
    "\n",
    "        #step3: multiply attention with value get results of attention:\n",
    "        #[64,6,12,10] * [64,6,10,50] = [64,6,12,50]\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # we have 12 words for query, we put 12 inthe front and put 50,6 at the end for calculation convenience:\n",
    "        # x [64,6,12,50] -> [64,12,6,50]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # concate multihead results:\n",
    "        # x = [64,12,6,50] -> [64,12,300]\n",
    "        x = x.view(bsz, -1, self.n_heads*(self.hid_dim//self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "query = torch.rand(64,12,300)\n",
    "key = torch.rand(64,10,300)\n",
    "value = torch.rand(64,10,300)\n",
    "\n",
    "attention = MultiHeadAttention(300,6,0.1)\n",
    "out = attention(query, key, value)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 300])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "81fee32150caccc2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
