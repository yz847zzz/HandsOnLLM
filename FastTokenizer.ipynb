{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T23:23:41.708047Z",
     "start_time": "2025-02-23T23:23:28.317191Z"
    }
   },
   "source": [
    "# only fast when parallel huge amount of text; in most case we just can use AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "example = 'Hello as hi'\n",
    "encoding = tokenizer(example)\n",
    "print(encoding)\n",
    "print('tokenizer.is_fast',tokenizer.is_fast)\n",
    "print('encoding.is_fast',encoding.is_fast)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 1112, 20844, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "tokenizer.is_fast True\n",
      "encoding.is_fast True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:33:45.531413Z",
     "start_time": "2025-02-23T23:33:45.425896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(encoding.tokens())"
   ],
   "id": "cb3e9fbcba2616f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:34:28.993731Z",
     "start_time": "2025-02-23T23:34:28.979730Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoding.word_ids())",
   "id": "c0f619580831bf8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:35:45.611338Z",
     "start_time": "2025-02-23T23:35:45.596825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_index = 5\n",
    "print('the 5th token is:', encoding.tokens()[token_index])\n",
    "start, end = encoding.token_to_chars(token_index)\n",
    "print('corresponding text span is:', example[start:end])\n",
    "word_index = encoding.word_ids()[token_index] # 3\n",
    "start, end = encoding.word_to_chars(word_index)\n",
    "print('corresponding word span is:', example[start:end])"
   ],
   "id": "bd41674009319a62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5th token is: ##yl\n",
      "corresponding text span is: yl\n",
      "corresponding word span is: Sylvain\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:47:20.678340Z",
     "start_time": "2025-02-23T23:47:20.664828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_index = 5\n",
    "print('the 5th token is:', encoding.tokens()[token_index])\n",
    "corresp_word_index = encoding.token_to_word(token_index)\n",
    "print('corresponding word index is:', corresp_word_index)\n",
    "start, end = encoding.word_to_chars(corresp_word_index)\n",
    "print('the word is:', example[start:end])\n",
    "start, end = encoding.word_to_tokens(corresp_word_index)\n",
    "print('corresponding tokens are:', encoding.tokens()[start:end])"
   ],
   "id": "3f341e22061d376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5th token is: ##yl\n",
      "corresponding word index is: 3\n",
      "the word is: Sylvain\n",
      "corresponding tokens are: ['S', '##yl', '##va', '##in']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:55:32.896910Z",
     "start_time": "2025-02-23T23:55:32.884399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = ' Sylvain is my name'\n",
    "print('characters of \"{}\" ars: {}'.format(chars, list(chars)))\n",
    "print('corresponding word index: ')\n",
    "for i, c in enumerate(chars):\n",
    "    #print(i, c)\n",
    "    print('\"{}\": {} '.format(c, encoding.char_to_word(i)), end=\"\")\n",
    "print('\\ncorresponding token index: ')\n",
    "#for i, c in enumerate(chars):\n",
    "    # print('\"{}\": {} '.format(c, encoding.char_to_token(i)), end=\"\")"
   ],
   "id": "d766f08d523c57e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters of \" Sylvain is my name\" ars: [' ', 'S', 'y', 'l', 'v', 'a', 'i', 'n', ' ', 'i', 's', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e']\n",
      "corresponding word index: \n",
      "\" \": 0 \"S\": 0 \"y\": None \"l\": 1 \"v\": 1 \"a\": 1 \"i\": 1 \"n\": None \" \": 2 \"i\": 2 \"s\": None \" \": 3 \"m\": 3 \"y\": 3 \" \": 3 \"n\": 3 \"a\": 3 \"m\": 3 \"e\": None \n",
      "corresponding token index: \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T23:58:36.183414Z",
     "start_time": "2025-02-23T23:58:35.413698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", grouped_entities = True)\n",
    "results = token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "print(results)"
   ],
   "id": "cf002343e6f847cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T00:01:10.461291Z",
     "start_time": "2025-02-24T00:01:10.074157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example,return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ],
   "id": "6fa43340aff0b6af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T00:01:43.039791Z",
     "start_time": "2025-02-24T00:01:43.025267Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.config.id2label)",
   "id": "27843c1811195105",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T00:04:56.350528Z",
     "start_time": "2025-02-24T00:04:56.339019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "\n",
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)"
   ],
   "id": "c6fa42b0144b4256",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T00:06:39.904979Z",
     "start_time": "2025-02-24T00:06:39.889962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "offset_mapping = inputs_with_offsets[\"offset_mapping\"]\n",
    "print(offset_mapping)"
   ],
   "id": "e77895f8506fecd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b4e00dfe3343ce3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
